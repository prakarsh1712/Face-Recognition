{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxjkxdI3aQUR"
      },
      "source": [
        "# Emotion Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDzI5mHA-UwV"
      },
      "source": [
        "## Install Requirements and Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A34LHga2Tiw",
        "outputId": "eae69e45-ca9a-4fed-e556-94e81d779f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive { vertical-output: true, display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTI_Du0H-f3_",
        "outputId": "05e972fa-a99a-4488-b213-5ac2b952bf30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries Installed\n"
          ]
        }
      ],
      "source": [
        "#@title Installing Libraries {vertical-output:true,display-mode:\"form\"}\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization,AveragePooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import h5py\n",
        "\n",
        "cas=cv2.CascadeClassifier(\"/content/drive/MyDrive/Project Exhibition-II/1.XML\")\n",
        "\n",
        "print('Libraries Installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjHzglvsnc2Y"
      },
      "source": [
        "### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH69jtVioKQM"
      },
      "outputs": [],
      "source": [
        "#@title BBox to Bytes {vertical-output:true,display-mode:\"form\"}\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okeNjOwXJG-H"
      },
      "outputs": [],
      "source": [
        "#@title WebCam Image Capture {vertical-output:true,display-mode:\"form\"}\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  img = js_to_image(data)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "#@title WebCam Video Stream {vertical-output:true,display-mode:\"form\"}\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovxXXdxIoD37"
      },
      "outputs": [],
      "source": [
        "#@title Js to CV2 {vertical-output:true,display-mode:\"form\"}\n",
        "def js_to_image(js_reply):\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_wllkZX_ghy"
      },
      "source": [
        "##V1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOEqSMpEl749"
      },
      "outputs": [],
      "source": [
        "#@title {vertical-output:true,display-mode:\"form\"}\n",
        "#@markdown Use WebCam for Capture?\n",
        "WEBCAM = True #@param {type:\"boolean\"}\n",
        "if WEBCAM :\n",
        "  img=take_photo()\n",
        "else:\n",
        "  #@markdown Enter the Address/Path of the Image File\n",
        "  IMAGE_PATH=\"/content/drive/MyDrive/Project Exhibition-II/test_manual/IMG_20221019_175625 (2).jpg\"#@param {type:\"string\"}\n",
        "  img=cv2.imread(IMAGE_PATH)\n",
        "\n",
        "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "fac=cas.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=8)\n",
        "\n",
        "for (x,y,w,h) in fac:\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "\n",
        "#@markdown Save Output to Google Drive (Default: false)?\n",
        "SAVE = False #@param {type:\"boolean\"}\n",
        "\n",
        "if(len(fac)>0):\n",
        "  print(\"Face Detected\")\n",
        "  if(SAVE):\n",
        "    cv2.imwrite('/content/drive/MyDrive/Project Exhibition-II/output/Face_Detected.jpg',img)\n",
        "  cv2_imshow(img)\n",
        "else:\n",
        "  print(\"No Face Detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhVv7spgvO1z"
      },
      "source": [
        "## v2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZJIJzHgdBKd"
      },
      "outputs": [],
      "source": [
        "#@title Trainer {vertical-output:true,display-mode:\"form\"}\n",
        "img_features = []\n",
        "img_label = []\n",
        "\n",
        "print(\"Training Started\")\n",
        "def create_train_data():\n",
        "    people = ['angry','disgust','fear', 'happy', 'neutral','sad','surprise']\n",
        "    directory = r\"/content/drive/MyDrive/Project Exhibition-II/train\"\n",
        "\n",
        "    for person in people:\n",
        "        path_for_person=os.path.join(directory,person)\n",
        "        img_labels=people.index(person)\n",
        "\n",
        "        for img in os.listdir(path_for_person):\n",
        "            img_path= os.path.join(path_for_person,img)\n",
        "\n",
        "            img_array=cv2.imread(img_path)\n",
        "            gray_image=cv2.cvtColor(img_array,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            faces_rect= cas.detectMultiScale(gray_image,scaleFactor=1.1,minNeighbors=2)\n",
        "\n",
        "            for (x,y,w,h) in faces_rect:\n",
        "                faces_roi=gray_image[y:y+h,x:x+w]\n",
        "                img_features.append(faces_roi)\n",
        "                img_label.append(img_labels)\n",
        "\n",
        "create_train_data()\n",
        "\n",
        "img_features=np.array(img_features,dtype='object')\n",
        "img_label=np.array(img_label)\n",
        "\n",
        "print(\"Size of Dataset: \", len(img_features))\n",
        "\n",
        "face_recogniszer=cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "face_recogniszer.train(img_features,img_label)\n",
        "\n",
        "face_recogniszer.save('/content/drive/MyDrive/Project Exhibition-II/face_trained.yml')\n",
        "\n",
        "np.save('/content/drive/MyDrive/Project Exhibition-II/features.npy',img_features)\n",
        "np.save('/content/drive/MyDrive/Project Exhibition-II/labels.npy',img_label)\n",
        "\n",
        "print(\"Trained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_xsU_-pp2M1",
        "outputId": "bb5b1975-1c77-4cfd-b3e9-85bd6e80a500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Load Trained Model {vertical-output:true,display-mode:\"form\"}\n",
        "features = np.load('/content/drive/MyDrive/Project Exhibition-II/features.npy',allow_pickle=True)\n",
        "labels = np.load('/content/drive/MyDrive/Project Exhibition-II/labels.npy')\n",
        "\n",
        "face_recogniszer=cv2.face.LBPHFaceRecognizer_create()\n",
        "face_recogniszer.read('/content/drive/MyDrive/Project Exhibition-II/face_trained.yml')\n",
        "\n",
        "people = ['angry','disgust','fear', 'happy', 'neutral','sad','surprise']\n",
        "\n",
        "print('Model Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwdS9tbRazXI"
      },
      "outputs": [],
      "source": [
        "#@title Image { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown Use WebCam for Capture?\n",
        "WEBCAM = True #@param {type:\"boolean\"}\n",
        "if WEBCAM:\n",
        "  img=take_photo()\n",
        "else:\n",
        "  #@markdown Enter the Address/Path of the Image File\n",
        "  IMAGE_PATH=\"/content/drive/MyDrive/Project Exhibition-II/test_manual/IMG_20221019_175625 (2).jpg\"#@param {type:\"string\"}\n",
        "  img=cv2.imread(IMAGE_PATH)\n",
        "\n",
        "imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=7)\n",
        "\n",
        "for (x,y,w,h) in fac:\n",
        "    faces_roi=imggray[y:y+h,x:x+w]\n",
        "    img_label,confidence=face_recogniszer.predict(faces_roi)\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "    cv2.putText(img,str(people[img_label]),(x,y),cv2.FONT_HERSHEY_COMPLEX_SMALL,1,(0,0,255),thickness=1)\n",
        "\n",
        "#@markdown Save Output to Google Drive (Default: false)?\n",
        "SAVE = False #@param {type:\"boolean\"}\n",
        "\n",
        "if(len(fac)>0):\n",
        "  print(\"Face Detected\")\n",
        "  if(SAVE):\n",
        "    cv2.imwrite('/content/drive/MyDrive/Project Exhibition-II/output/Face_Detected.jpg',img)\n",
        "  cv2_imshow(img)\n",
        "else:\n",
        "  print(\"No Face/Emotion Detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CACJ0M5mq3Xp",
        "outputId": "10f0c699-3058-4515-bf4f-3822248d4206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emotion Detected\n"
          ]
        }
      ],
      "source": [
        "#@title Video { vertical-output: true, display-mode: \"form\" }\n",
        "cas=cv2.CascadeClassifier(\"/content/drive/MyDrive/Project Exhibition-II/1.XML\")\n",
        "\n",
        "#@markdown Enter the Address/Path of the Video File\n",
        "VIDEO_PATH=\"/content/drive/MyDrive/Project Exhibition-II/test_manual/WhatsApp Video 2023-01-18 at 15.44.37.mp4\"#@param {type:\"string\"}\n",
        "cap=cv2.VideoCapture(VIDEO_PATH)\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video_out = cv2.VideoWriter('/content/drive/MyDrive/Project Exhibition-II/output/Emotion_Detected_Vid.mp4', fourcc, 24, (w, h))\n",
        "detected=False\n",
        "\n",
        "success,img=cap.read()\n",
        "while success:\n",
        "    imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=3)\n",
        "    \n",
        "    if(len(fac)>0): detected=True\n",
        "    \n",
        "    for (x,y,w,h) in fac:\n",
        "        faces_roi=imggray[y:y+h,x:x+w]\n",
        "        img_lable,confidence=face_recogniszer.predict(faces_roi)\n",
        "        cv2.putText(img,str(people[img_lable]),(20,20),cv2.FONT_HERSHEY_COMPLEX,1.0,(0,255,0),thickness=2)\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "    video_out.write(img)\n",
        "    success,img=cap.read()\n",
        "if(detected):\n",
        "    video_out.release()\n",
        "    print(\"Emotion Detected\")\n",
        "else:\n",
        "    print(\"No Emotion Detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UbHE8-DnyGD"
      },
      "outputs": [],
      "source": [
        "#@title WebCam Feed { vertical-output: true , display-mode: \"form\" }\n",
        "#@markdown Save WebCam Feed as video file (Default: false)?\n",
        "SAVE = True #@param {type:\"boolean\"}\n",
        "video_stream()\n",
        "label_html='No Emotion Detected'\n",
        "bbox = ''\n",
        "\n",
        "if SAVE:\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  video = cv2.VideoWriter('/content/drive/MyDrive/Project Exhibition-II/output/Emotion_Detected_WebCam.mp4', fourcc, 24, (640, 480))\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=3)\n",
        "\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    \n",
        "    for (x,y,w,h) in fac:\n",
        "      faces_roi=imggray[y:y+h,x:x+w]\n",
        "      img_lable,confidence=face_recogniszer.predict(faces_roi)\n",
        "      bbox_array = cv2.putText(bbox_array,str(people[img_lable]),(20,20),cv2.FONT_HERSHEY_COMPLEX,1.0,(0,255,0),thickness=2)\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "      if SAVE:\n",
        "        cv2.putText(img,str(people[img_lable]),(20,20),cv2.FONT_HERSHEY_COMPLEX,1.0,(0,255,0),thickness=2)\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "    \n",
        "    if SAVE:\n",
        "      video.write(img)\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    \n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    \n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    if(len(fac)>0): label_html = 'Emotion Detected'\n",
        "    else: label_html='No Emotion Detected'\n",
        "\n",
        "if SAVE:\n",
        "  video.release()\n",
        "  print(\"WebCam feed saved successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPaHjdPU7lgi"
      },
      "source": [
        "## V3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XpByzw-dtyZ"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Extraction {vertical-output:true, display-mode:\"form\"}\n",
        "!unzip /content/archive_angry.zip -d /content/angry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4lXHhqQVzPpT",
        "outputId": "838d83d9-5ff5-4e68-af8f-f230f7c0d197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 48, 48, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 48, 48, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_4 (Averag  (None, 24, 24, 64)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 24, 24, 128)       204928    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_5 (Averag  (None, 12, 12, 128)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 12, 12, 512)       590336    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 12, 12, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_6 (Averag  (None, 6, 6, 512)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 6, 6, 512)         0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 6, 6, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_7 (Averag  (None, 3, 3, 512)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               1179904   \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,496,390\n",
            "Trainable params: 4,492,422\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a48bd350d830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m validation_datagen = ImageDataGenerator(rescale = 1./255,\n\u001b[1;32m     48\u001b[0m                                          validation_split = 0.2)\n\u001b[0;32m---> 49\u001b[0;31m train_generator = train_datagen.flow_from_directory(directory = train_dir,\n\u001b[0m\u001b[1;32m     50\u001b[0m                                                     \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train'"
          ]
        }
      ],
      "source": [
        "#@title Trainer {vertical-output:true, display-mode:\"form\"}\n",
        "train_dir = \"/content/train\"\n",
        "test_dir = \"/content/test\"\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='tanh', input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (5, 5), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.75))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "img_size=48\n",
        "\n",
        "train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n",
        "                                         height_shift_range = 0.1,\n",
        "                                         horizontal_flip = True,\n",
        "                                         rescale = 1./255,\n",
        "                                         validation_split = 0.2\n",
        "                                        )\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                         validation_split = 0.2)\n",
        "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
        "                                                    target_size = (img_size,img_size),\n",
        "                                                    batch_size = 32,\n",
        "                                                    color_mode = \"grayscale\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    subset = \"training\",\n",
        "                                                    shuffle = True\n",
        "                                                   )\n",
        "validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n",
        "                                                              target_size = (img_size,img_size),\n",
        "                                                              batch_size = 32,\n",
        "                                                              color_mode = \"grayscale\",\n",
        "                                                              class_mode = \"categorical\",\n",
        "                                                              subset = \"validation\",\n",
        "                                                              shuffle = True\n",
        "                                                             )\n",
        "model.compile(\n",
        "    optimizer=Adam(lr=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "#@markdown Provide the name for the trained model.\n",
        "MODEL_NAME=\"model_angry\"#@param {type:\"string\"}\n",
        "es=EarlyStopping(monitor=\"accuracy\",min_delta=0.0001,patience=20,verbose=1,restore_best_weights=True)\n",
        "mp=ModelCheckpoint(filepath=MODEL_NAME+\".h5\",monitor=\"accuracy\",save_best_only=True,verbose=1)\n",
        "es4=EarlyStopping(monitor=\"val_accuracy\",min_delta=0.0001,patience=20,verbose=1,restore_best_weights=True)\n",
        "mp4=ModelCheckpoint(filepath=MODEL_NAME+\".h5\",monitor=\"val_accuracy\",save_best_only=True,verbose=1)\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "history = model.fit(x=train_generator,epochs = epochs,validation_data = validation_generator,callbacks=[es,mp,es4,mp4])\n",
        "\n",
        "model_save=model.to_json()\n",
        "with open(\"/content/\"+MODEL_NAME+\".json\",\"w\") as json_file:\n",
        "    json_file.write(model_save)\n",
        "\n",
        "model.save_weights('/content/emotion_'+MODEL_NAME+'.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJD7QcOORPs6",
        "outputId": "38e645a1-e8c0-4a5d-a04c-731d15ecaedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded\n"
          ]
        }
      ],
      "source": [
        "#@title Load Trained Model {vertical-output: true, display-mode: \"form\"}\n",
        "#@markdown Enter name of model to be used.\n",
        "MODEL=\"model6\"#@param {type:\"string\"}\n",
        "json_file=open('/content/'+MODEL+'.json','r')\n",
        "loded_file=json_file.read()\n",
        "json_file.close()\n",
        "emotion_model=model_from_json(loded_file)\n",
        "\n",
        "emotion_model.load_weights('/content/emotion_'+MODEL+'.h5')\n",
        "emotionsdictionary={0:\"angry\",1:\"fear\",2:\"happy\",3:\"neutral\",4:\"sad\",5:\"surprise\"}\n",
        "print(\"Model Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7JhkXBDYAs"
      },
      "outputs": [],
      "source": [
        "#@title Image {vertical-output:true, display-mode: \"form\"}\n",
        "#@markdown Use WebCam for Capture?\n",
        "WEBCAM = False #@param {type:\"boolean\"}\n",
        "if WEBCAM:\n",
        "  img=take_photo()\n",
        "else:\n",
        "  #@markdown Enter the Address/Path of the Image File\n",
        "  IMAGE_PATH=\"/content/download.jpeg\"#@param {type:\"string\"}\n",
        "  img=cv2.imread(IMAGE_PATH)\n",
        "\n",
        "imggray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=7)\n",
        "\n",
        "\n",
        "for (x,y,w,h) in fac:\n",
        "    roi_gray_frame=imggray[y:y+h,x:x+w]\n",
        "    cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)),-1),0)\n",
        "    emotionpredict = emotion_model.predict(cropped_img)\n",
        "    maxindex=int(np.argmax(emotionpredict))\n",
        "    faces_roi=imggray[y:y+h,x:x+w]\n",
        "    emotionpredict=emotion_model.predict(cropped_img)\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "    cv2.putText(img,emotionsdictionary[maxindex],(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),thickness=2)\n",
        "\n",
        "#@markdown Save Output to Google Drive (Default: false)?\n",
        "SAVE = True #@param {type:\"boolean\"}\n",
        "\n",
        "if(len(fac)>0):\n",
        "  print(\"Face Detected\")\n",
        "  if(SAVE):\n",
        "    cv2.imwrite('/content/drive/MyDrive/Project Exhibition-II/output/Face_Detected.jpg',img)\n",
        "  cv2_imshow(img)\n",
        "else:\n",
        "  print(\"No Face/Emotion Detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBkBCrg8SeBw"
      },
      "outputs": [],
      "source": [
        "#@title Video { vertical-output: true, display-mode: \"form\" }\n",
        "cas=cv2.CascadeClassifier(\"/content/drive/MyDrive/Project Exhibition-II/1.XML\")\n",
        "\n",
        "#@markdown Enter the Address/Path of the Video File\n",
        "VIDEO_PATH=\"/content/drive/MyDrive/Project Exhibition-II/test_manual/WhatsApp Video 2023-01-18 at 15.44.37.mp4\"#@param {type:\"string\"}\n",
        "cap=cv2.VideoCapture(VIDEO_PATH)\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video = cv2.VideoWriter('/content/drive/MyDrive/Project Exhibition-II/output/Emotion_Detected_Tensorflow_Vid.mp4', fourcc, 24, (w, h))\n",
        "detected=False\n",
        "\n",
        "success,img=cap.read()\n",
        "while success:\n",
        "    imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=3)\n",
        "    \n",
        "    if(len(fac)>0): detected=True\n",
        "    \n",
        "    for (x,y,w,h) in fac:\n",
        "        roi_gray_frame=imggray[y:y+h,x:x+w]\n",
        "        cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)),-1),0)\n",
        "        emotionpredict = emotion_model.predict(cropped_img)\n",
        "        maxindex=int(np.argmax(emotionpredict))\n",
        "        faces_roi=imggray[y:y+h,x:x+w]\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "        cv2.putText(img,emotionsdictionary[maxindex],(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),thickness=2)\n",
        "    video.write(img)\n",
        "    success,img=cap.read()\n",
        "if(detected):\n",
        "    print(\"Emotion Detected\")\n",
        "    video.release()\n",
        "else:\n",
        "    print(\"No Emotion Detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "vZvf4ZJ8TdOh",
        "outputId": "46e445e8-6b31-4619-bcc7-c3e189750447"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title WebCam Feed { vertical-output: true , display-mode: \"form\" }\n",
        "#@markdown Save WebCam Feed as video file (Default: false)?\n",
        "SAVE = False #@param {type:\"boolean\"}\n",
        "video_stream()\n",
        "label_html='No Emotion Detected'\n",
        "bbox = ''\n",
        "\n",
        "if SAVE:\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  video = cv2.VideoWriter('/content/drive/MyDrive/Project Exhibition-II/output/Emotion_Detected_Tensorflow_WebCam.mp4', fourcc, 24, (640, 480))\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    imggray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    fac=cas.detectMultiScale(imggray,scaleFactor=1.1,minNeighbors=9)\n",
        "\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    \n",
        "    for (x,y,w,h) in fac:\n",
        "      roi_gray_frame=imggray[y:y+h,x:x+w]\n",
        "      cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)),-1),0)\n",
        "      emotionpredict = emotion_model.predict(cropped_img,verbose=0)\n",
        "      maxindex=int(np.argmax(emotionpredict))\n",
        "      faces_roi=imggray[y:y+h,x:x+w]\n",
        "      bbox_array = cv2.putText(bbox_array,emotionsdictionary[maxindex],(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),thickness=2)\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "      if SAVE:\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
        "        cv2.putText(img,emotionsdictionary[maxindex],(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),thickness=2)\n",
        "    \n",
        "    if SAVE:\n",
        "      video.write(img)\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    \n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    \n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    if(len(fac)>0): label_html = 'Emotion Detected'\n",
        "    else: label_html='No Emotion Detected'\n",
        "\n",
        "if SAVE:\n",
        "  video.release()\n",
        "  print(\"WebCam feed saved successfully\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjHzglvsnc2Y"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}